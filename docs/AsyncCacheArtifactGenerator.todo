Async Cache Artifact Generator (Generic Regen Queue + Disk + GPU)

Goal:
  Build a reusable, abstracted system that manages a regeneration task queue for any cache/artifact type
  (e.g. BaseColor, MaterialParams, NormalDepth) and continuously executes those tasks.

User decisions (locked in):
  - Thread policy: Option A (load + decode assets on worker threads).
  - Persistence policy: Option B (no dropped writes; enforce backpressure, but never block the render thread).
  - GPU uploads should be “async” where possible by using existing GPU wrappers:
    worker threads can fill already-mapped staging memory (persistent PBO ring), while actual GL commits
    remain on the render thread via the TextureStreaming/GpuResourceManager service loop.

Reference docs (read first):
  - docs/MaterialAtlas.Architecture.md (threading model + session cancellation)
  - docs/MaterialAtlas.BuildPipeline.md (budgeted async execution patterns)
  - docs/MaterialSystem.Cache.Architecture.md (cache layers, stores, key schema)

Reference code (patterns to reuse):
  - VanillaGraphicsExpanded/PBR/Materials/PbrMaterialRegistry.cs (BaseColor regen: session-id stale drop + main-thread apply)
  - VanillaGraphicsExpanded/PBR/Materials/Async/* (Material atlas job scheduling + budgets)
  - VanillaGraphicsExpanded/Rendering/TextureStreamingManager.cs (persistent mapped staging + render-thread commit)
  - VanillaGraphicsExpanded/Rendering/GpuTexture.cs (wrapper + deferred disposal)


State Machine (Concrete)

Definitions:
  - `SessionId`: monotonic int/long incremented on Start() and on texture reload.
  Any result produced under an older SessionId is stale and must be dropped.
  - `Key`: the cache key (e.g., AtlasCacheKey for BaseColor, tile key for atlas artifacts).
  - `Outputs`: each work item may produce 0..2 outputs:
    (A) Disk output: persisted via ICacheStore/codec.
    (B) GPU output: staged off-thread then committed on render thread.

States (single work item instance; key is the identity):
  S0  Idle
    - Not present in queue, not in-flight.

  S1  Queued
    - Enqueued in priority FIFO.
    - De-dup invariant: at most one queued instance per Key.

  S2  Admitted
    - Chosen for execution by scheduler.
    - Admission is where Option B backpressure is enforced.
    - Requires acquiring any needed reservation tokens before leaving this state.

  S3  Computing (Worker)
    - Worker thread loads assets + decodes + computes.
    - Produces DiskPayload and/or GpuPayload in memory.
    - May also determine “noop” (already up-to-date / cache hit).

  S4  DiskWriting (Worker)
    - Off-thread disk persistence.
    - Must not be dropped (Option B): admission ensures capacity exists.

  S5  GpuStaging (Worker)
    - Off-thread staging only (memcpy into already-mapped staging memory).
    - No GL calls. Uses TextureStreaming/GpuTexture wrappers.

  S6  GpuCommitQueued (Render)
    - A commit/upload command is queued for render-thread execution.
    - Actual GL calls happen during the existing render-thread service loop.

  S7  Applying (Main/Render)
    - Apply results back to gameplay-visible structures (e.g., in-memory maps).
    - Main-thread apply uses Event.EnqueueMainThreadTask.
    - Render-thread apply is permitted for GPU object bookkeeping only.

  S8  Completed
    - Work item finished successfully for the current SessionId.
    - Reservations are released.

  S9  Failed
    - Work item failed. Release reservations.
    - Retry policy decides if/how it re-enters S1.

  SX  StaleDropped (Terminal)
    - Any state → SX if SessionId mismatch is detected before Apply.
    - Must release reservations and avoid mutating shared state.

Transitions (high-level):
  T0: Enqueue(Key)                S0 → S1
  T1: Scheduler selects Key       S1 → S2

  T2: Acquire reservations        S2 → S3
    - DiskReservation required iff the work item will use disk output.
    - GpuReservation required iff the work item will use GPU output (ties to streaming budgets/limits).
    - If reservation not available: defer (S2 → S1) without blocking render thread.

  T3: Compute yields outputs      S3 → (S4 and/or S5 and/or S7 and/or S8)
    - If both outputs: sequence is S4 then S5 then S6 then S7 (exact ordering depends on consumer).
    - If cache hit and no apply needed: S3 → S8.

  T4: Disk write completes        S4 → next
  T5: GPU stage completes         S5 → S6
  T6: Render commit executes      S6 → next (optionally signals fence/ack)
  T7: Apply on main/render        next → S7 → S8

  T8: Error                        any → S9
  T9: Session mismatch             any → SX

Invariants:
  - Never block render thread. Reservations are acquired on scheduler/worker context.
  - No GL calls off-thread; only staging memcpy is allowed off-thread.
  - All shared-state mutation must pass SessionId gate.



✔ Phase 0 - Design & Contracts
  Exit artifacts: clear interfaces, thread-affinity contract, and backpressure semantics.
  ✔ Define terms:
    - “Work item” (key + priority + dependencies)
    - “Artifact outputs” (disk payload, GPU payload)
    - “Session” (session id + cancellation token)
  ☐ Define core interfaces (names tentative):
    - `IArtifactWorkItem<TKey>` (key, priority, type id, debug label)
    - `IArtifactComputer<TKey, TOutput>` (worker-thread compute)
    - `IArtifactOutputStage<TKey, TOutput>` (single post-compute output stage; may dispatch disk + GPU work)
    - `IArtifactApplier<TKey, TOutput>` (main-thread apply, optional)
    - `IArtifactDiskStore<TKey, TDiskPayload>` (off-thread write)
    - `IArtifactGpuStore<TKey, TGpuPayload>` (off-thread stage + render-thread commit)
    - `IArtifactScheduler` (Start/Stop/Enqueue/DrainTick/GetStatsSnapshot)
  ✔ Specify thread-affinity explicitly:
    - Worker threads: asset load + decode + compute
    - Disk writes: off-thread
    - GPU staging: off-thread (only memcpy into mapped staging memory)
    - GPU commit: render thread only
  ✔ Define backpressure rules (Option B):
    - Never drop writes.
    - Never block render thread.
    - If no IO/GPU capacity is available, defer (don’t schedule) work.


✔ Phase 1 - Core Scheduler (Queue + Lifecycle)
  Exit artifacts: a generic scheduler that owns the work queue and continuously executes.
  ✔ Implement queue primitive:
    - Priority FIFO (reuse or adapt `PriorityFifoQueue` patterns)
    - De-dup by key (avoid infinite requeue)
  ✔ Implement lifecycle:
    - Start after preload/warmup
    - Stop on reload/dispose
    - Session id increments per start; stale results are dropped (see State Machine: SX)
  ✔ Implement continuous execution loop:
    - Bounded worker concurrency (SemaphoreSlim)
    - Cooperative cancellation
    - Small yields (avoid starving other tasks)
  ✔ Add stats snapshot:
    - queued, in-flight, completed, errors
    - timings: avg compute, avg output


✔ Phase 2 - Backpressure + Reservations (Disk + GPU)
  Exit artifacts: producers cannot overrun disk/GPU pipelines; no dropped writes.
  ✔ Disk write backpressure:
    - Define an IO “reservation token” (capacity N)
    - Reserve during Admission (S2→S3) before scheduling compute that will produce disk output
    - Release after write completes
  ✔ GPU upload backpressure:
    - Do not invent a second render-thread pipeline; integrate with texture streaming budgets/queues
    - Add a “can stage/can commit” check or reservation tied to existing streaming limits (S2 gate)
  ✔ Prove “never block render thread”:
    - Reservations happen on worker/scheduler thread only
    - Render-thread commit work is bounded and budgeted


✔ Phase 3 - GPU Output Integration (Async Staging + Render Commit)
  Exit artifacts: generic path for GPU payloads via existing wrappers.
  ✔ Define GPU payload types:
    - Raw byte payload (e.g., tex sub-image bytes)
    - Typed payload (e.g., float arrays) that can be converted to staging bytes
  ✔ Implement GPU uploader adapter(s):
    - `TextureStreamingSystem.StageCopy(...)` based staging (preferred)
    - `GpuTexture` wrapper path (when texture object ownership is required)
  ✔ Handle readiness:
    - Before streaming manager initializes, staging falls back to CPU buffers (still correct)
    - Document the perf implication
  ☐ Optional: fence/ack handling if “completion” is required for reuse


✔ Phase 4 - First Consumer: BaseColor Generator
  Exit artifacts: BaseColor regen uses the generic scheduler (no bespoke loop).
  ✔ Implement `BaseColorArtifactGenerator`:
    - Worklist: canonical texture ids (registry mappings)
    - Compute: load asset + ToBitmap + average linear RGB
    - Disk payload: BaseColor RGB16F JSON dictionary entry
    - Apply: update in-memory map on main thread
  ✔ Replace/retire bespoke BaseColor regen loop in `PbrMaterialRegistry`.
  ✔ Validate:
    - session cancellation drops stale results
    - disk writes are not dropped under load


✔ Phase 5 - Material Atlas Integration (MaterialParams + NormalDepth)
  Exit artifacts: atlas generation uses shared scheduler semantics while preserving render-thread constraints.
  ✔ Identify atlas pipeline seams:
    - CPU jobs (compute, encode)
    - GPU jobs (upload/commit)
    - Disk cache IO queue (currently drop-on-full)
  ✔ Convert atlas disk writes to Option B semantics:
    - Reserve IO slots upstream; do not drop
    - Ensure render-thread never waits
  ✔ Convert GPU uploads to generic GPU payload path where feasible:
    - Prefer StageCopy / persistent mapped staging
    - Keep GL commits on render thread


✔ Phase 6 - Validation, Diagnostics, and Docs
  Exit artifacts: tests, logs/debug views, and documented behavior.
  ✔ Tests:
    - corrupt entry recovery doesn’t poison load
    - session cancellation drops stale results
    - backpressure: writes are not dropped; producer defers instead
  ✔ Diagnostics:
    - summary log lines per run (scheduled/completed/errors, avg compute/IO)
    - optional debug view panel for per-cache-type progress
  ✔ Docs:
    - document thread-affinity contract
    - document backpressure semantics and why render thread never blocks
    - document how GPU staging works (mapped PBO writes off-thread, GL commit on render thread)
