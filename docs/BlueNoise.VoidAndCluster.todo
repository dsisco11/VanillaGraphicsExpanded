# Blue Noise: Runtime Compute + Cache (Void-and-Cluster)

Goal: Implement a runtime system to compute and cache reusable blue-noise arrays.
Scope: CPU-side generation + cache + configurable outputs. Downstream consumer integration is explicitly out-of-scope.

Key requirements
- Tiling is optional, default `true`.
- Output type is configurable.
- Sizes are configurable.
- Configuration type is a `readonly record struct`.
- Algorithm: Void-and-Cluster (Ulichney).
- Prefer `System.Numerics.Tensors` (`TensorPrimitives`) for bulk operations wherever it makes sense.

---

## Phase 0 — References / Reading
- [x] Confirm exact Void-and-Cluster procedure steps and ranking method (avoid “almost correct” implementations).
  - Practical, concise writeup + discussion: https://momentsingraphics.de/BlueNoise.html
  - Reference implementation (matches Ulichney 1993, includes the 3-phase ranking procedure): https://raw.githubusercontent.com/MomentsInGraphics/BlueNoise/master/BlueNoise.py
  - Original paper (primary source): R. A. Ulichney (1993), “Void-and-cluster method for dither array generation” (Proc. SPIE 1913).
- [x] Identify a concise summary to link in docs (paper, notes, or community writeup).
  - Use https://momentsingraphics.de/BlueNoise.html as the main link; it’s readable and directly covers Void-and-Cluster.
- [x] Review tensor APIs we’ll lean on:
  - https://learn.microsoft.com/dotnet/api/system.numerics.tensors
  - https://learn.microsoft.com/dotnet/api/system.numerics.tensors.tensorprimitives

---

## Phase 1 — Public API + Configuration
- [x] Add `readonly record struct BlueNoiseConfig` (Noise namespace).
  - [x] Fields: `Width`, `Height` (and optional `Depth/Slices` if needed later).
  - [x] `Tileable` default `true`.
  - [x] `Seed` (uint) and `Algorithm` (enum or type tag).
  - [x] Output selection: `BlueNoiseOutputKind` (enum).
  - [x] Algorithm parameters: `Sigma`, `InitialFillRatio`, `MaxIterations`, `StagnationLimit`.
- [x] Add validation helper (throws or returns error result): sizes > 0, sigma > 0, fill in (0,1), etc.
- [x] Define the canonical artifact to cache:
  - [x] Ranked threshold map (e.g., `ushort[]` length = width*height) should likely be the “source of truth”.

---

## Phase 2 — Cache Design (CPU)
- [x] Add `BlueNoiseKey` as a `readonly record struct` suitable as a dictionary key.
  - [x] Include: size, tileable, seed, algorithm id, sigma (cache ranked only; output kind conversion will be on-demand).
- [x] Implement `BlueNoiseCache` (thread-safe) with “create once” semantics.
  - [x] Prefer `ConcurrentDictionary<BlueNoiseKey, Lazy<T>>` or a small `lock`-protected dictionary.
  - [x] Ensure generation work is not done under a long-held lock.
- [x] Decide eviction strategy:
  - [x] Default: no eviction (expect a small fixed set of configs).
  - [x] Optional: bounded LRU for unusual/experimental configs.

---

## Phase 3 — Core Void-and-Cluster Implementation
- [x] Implement `VoidAndClusterGenerator` producing a ranked threshold map.
  - [x] Deterministic RNG: seed with existing stable hashing + Squirrel3-style hash PRNG.
  - [x] Initial binary pattern generation at `InitialFillRatio`.
  - [x] Compute “density” field via Gaussian blur of the binary pattern.
  - [x] Iteratively swap the most “clustered” ON pixel with the most “void” OFF pixel.
    - [x] Define convergence criteria (iteration cap + stagnation).
    - [x] Ensure toroidal addressing when tileable.
  - [x] Ranking step (generate dither matrix thresholds):
    - [x] Confirm Ulichney’s ranking method precisely.
    - [x] Produce ranks `0..(N-1)` (or inverse) consistent with output conversions.
- [x] Boundary behavior:
  - [x] Tileable: toroidal wrap in both axes for all neighborhood/blur ops.
  - [x] Non-tileable: clamp edges consistently.

---

## Phase 4 — Gaussian Blur (Separable) + Tensor Usage
- [x] Build a separable Gaussian kernel from `Sigma`.
  - [x] Choose radius rule (e.g., 3*sigma) and ensure odd kernel length.
  - [x] Normalize kernel sum to 1.
- [x] Implement separable convolution:
  - [x] Horizontal pass into scratch buffer.
  - [x] Vertical pass into output buffer.
- [x] Apply `TensorPrimitives` where it helps:
  - [x] Use `TensorPrimitives.Dot` for inner products in the sliding window implementation.
- [x] Add scratch buffer reuse:
  - [x] Use `ArrayPool<float>` and `ArrayPool<int>` for convolution scratch/indices.

---

## Phase 5 — Output Kinds + Conversions
- [x] Define `BlueNoiseOutputKind` (examples):
  - [x] `RankU16` (canonical / cacheable).
  - [x] `RankU32` (only if required).
  - [x] `NormalizedF32` in [0,1].
  - [x] `L8` byte in [0,255].
  - [x] `BinaryMask` given a threshold or fill ratio.
- [x] Implement conversion pipeline from ranked map to requested output:
  - [x] Use `TensorPrimitives` conversions and math (`ConvertTruncating`, `Multiply`, `Add`, `Clamp`).
  - [x] Validate monotonic mapping and range correctness (range clamps + argument validation).

---

## Phase 6 — Determinism + Correctness Tests
- [ ] Add unit tests (no visual output required):
  - [ ] Deterministic: same config produces identical results.
  - [ ] Different seed differs.
  - [ ] Tileable: edge continuity checks (wrap neighborhood / periodic boundary).
  - [ ] Output correctness: range checks, size checks, no NaNs.
  - [ ] Cache behavior: repeated requests reuse cached artifact.

---

## Phase 7 — Performance Validation
- [ ] Measure generation time for common sizes (64, 128, 256) and typical sigmas.
- [ ] Confirm no excessive allocations (pool usage effective).
- [ ] Document recommended “safe defaults” for game startup vs lazy generation.

---

## Phase 8 — Documentation + Future Algorithms (Non-blocking)
- [ ] Add a short doc page describing:
  - [ ] What the cache provides (rank map, normalized map, mask).
  - [ ] How tiling affects boundary behavior.
  - [ ] How to pick sigma / size / iteration caps.
- [ ] Future algorithm candidates (not required now):
  - [ ] Progressive Multi-Jittered (PMJ) sequences for temporal jitter textures.
  - [ ] Bridson Poisson-disk sampling for point sets.
  - [ ] Owen-scrambled Sobol (not blue-noise, but excellent low-discrepancy sampling).
